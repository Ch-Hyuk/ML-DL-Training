{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import configparser\n",
    "import speech_recognition as sr\n",
    "import pyaudio\n",
    "import wave\n",
    "import keyboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#API key setting\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "openai.api_key = config.get('api', 'api_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-3 API request\n",
    "\n",
    "def GPT_API(input):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt= input,\n",
    "        max_tokens=200\n",
    "    )\n",
    "\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "# Whisper API request\n",
    "def Whisper_API(input_path):\n",
    "    audio_file = open(input_path, 'rb')\n",
    "    transcript = openai.Audio.transcribe('whisper-1', audio_file)\n",
    "    \n",
    "    return transcript['text']\n",
    "\n",
    "#voice recode\n",
    "\n",
    "def voice_recode():\n",
    "    FORMAT = pyaudio.paInt16  \n",
    "    CHANNELS = 1  \n",
    "    RATE = 44100  \n",
    "    CHUNK = 1024  \n",
    "    OUTPUT_DIRECTORY = \"record\"\n",
    "    OUTPUT_FILENAME = os.path.join(OUTPUT_DIRECTORY, \"recorded_audio_ver1.wav\")\n",
    "\n",
    "    audio = pyaudio.PyAudio()\n",
    "\n",
    "    stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
    "                    rate=RATE, input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "\n",
    "    print(\"Recording Start --> (if press 'q', quit recording )\")\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    while not keyboard.is_pressed('q'):\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "\n",
    "    print(\"Recording Stop!\")\n",
    "\n",
    "    # 스트림과 PyAudio 객체 닫기\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "\n",
    "    # 녹음된 데이터를 WAV 파일로 저장\n",
    "    with wave.open(OUTPUT_FILENAME, 'wb') as wf:\n",
    "        wf.setnchannels(CHANNELS)\n",
    "        wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "        wf.setframerate(RATE)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "\n",
    "    print(f\"녹음된 파일 {OUTPUT_FILENAME} 저장 완료.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_recode()\n",
    "text = Whisper_API('./record/recorded_audio.wav')\n",
    "result = GPT_API(text)\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Aivenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
