{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적의 단어 순서 배열: ('예제', '최적의', '단어', '순서', '배열', '찾기')\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# 텍스트 데이터 (예제)\n",
    "text = \"\"\"\n",
    "이것은 예제 문장입니다. 예제 문장을 가지고 N-gram과 마르코프 체인을 학습합니다.\n",
    "여러개의 단어가 주어지면 최적의 단어 순서 배열을 찾아야 합니다.\n",
    "이를 위해 다양한 방법을 사용할 수 있습니다.\n",
    "\"\"\"\n",
    "\n",
    "# 전처리 및 토큰화\n",
    "tokens = text.split()\n",
    "\n",
    "# N-gram 모델 생성 (여기서는 Trigram 예제)\n",
    "trigrams = [(tokens[i], tokens[i+1], tokens[i+2]) for i in range(len(tokens)-2)]\n",
    "trigram_freq = Counter(trigrams)\n",
    "\n",
    "# 마르코프 체인 기반 전이 확률 테이블 생성\n",
    "transitions = defaultdict(lambda: defaultdict(int))\n",
    "for (w1, w2, w3) in trigrams:\n",
    "    transitions[(w1, w2)][w3] += 1\n",
    "\n",
    "# 전이 확률 계산\n",
    "for prev_words, next_words in transitions.items():\n",
    "    total = float(sum(next_words.values()))\n",
    "    for word in next_words:\n",
    "        next_words[word] /= total\n",
    "\n",
    "# 최적의 단어 순서 배열 선택 함수\n",
    "def score_sequence(sequence, transitions):\n",
    "    score = 0.0\n",
    "    for i in range(len(sequence) - 2):\n",
    "        w1, w2, w3 = sequence[i], sequence[i+1], sequence[i+2]\n",
    "        if (w1, w2) in transitions and w3 in transitions[(w1, w2)]:\n",
    "            score += transitions[(w1, w2)][w3]\n",
    "        else:\n",
    "            score -= 1  # 낮은 확률로 패널티 부여\n",
    "    return score\n",
    "\n",
    "def find_best_sequence(words, transitions):\n",
    "    best_sequence = None\n",
    "    best_score = float('-inf')\n",
    "    for perm in itertools.permutations(words):\n",
    "        score = score_sequence(perm, transitions)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_sequence = perm\n",
    "    return best_sequence\n",
    "\n",
    "# 주어진 단어들\n",
    "words = [\"예제\", \"최적의\", \"단어\", \"순서\", \"배열\", \"찾기\"]\n",
    "\n",
    "# 최적의 단어 순서 배열 찾기\n",
    "best_sequence = find_best_sequence(words, transitions)\n",
    "print(\"최적의 단어 순서 배열:\", best_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best sequence: ('기술', '안전', '분석', '데이터')\n",
      "Probability: 0.078125\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# 예시 데이터 준비\n",
    "documents = [\n",
    "    [\"기술\", \"안전\", \"분석\", \"데이터\"],\n",
    "    [\"공정\", \"관리\", \"품질\", \"생산\"],\n",
    "    [\"안전\", \"기술\", \"분석\", \"공정\"],\n",
    "    [\"데이터\", \"관리\", \"분석\", \"품질\"]\n",
    "]\n",
    "\n",
    "# 모든 가능한 단어 조합 생성\n",
    "def generate_sequences(documents):\n",
    "    sequences = []\n",
    "    for terms in documents:\n",
    "        for i in range(1, len(terms)):\n",
    "            for j in range(len(terms) - i + 1):\n",
    "                seq = terms[j:j+i+1]\n",
    "                sequences.append(seq)\n",
    "    return sequences\n",
    "\n",
    "sequences = generate_sequences(documents)\n",
    "\n",
    "# N-gram 모델 생성\n",
    "n = 2  # 예를 들어 Bigram을 사용\n",
    "ngrams = defaultdict(Counter)\n",
    "\n",
    "for sequence in sequences:\n",
    "    for i in range(len(sequence) - n + 1):\n",
    "        gram = tuple(sequence[i:i+n-1])\n",
    "        next_word = sequence[i + n - 1]\n",
    "        ngrams[gram][next_word] += 1\n",
    "\n",
    "# 단어 배치 확률 계산 함수\n",
    "def calculate_sequence_probability(sequence, ngrams):\n",
    "    prob = 1.0\n",
    "    for i in range(len(sequence) - 1):\n",
    "        current_gram = tuple(sequence[i:i+n-1])\n",
    "        next_word = sequence[i + n - 1]\n",
    "        next_word_count = ngrams[current_gram][next_word]\n",
    "        total_count = sum(ngrams[current_gram].values())\n",
    "        prob *= next_word_count / total_count if total_count > 0 else 0\n",
    "    return prob\n",
    "\n",
    "# 최적의 단어 배치 찾기 함수\n",
    "def find_best_sequence(words, ngrams):\n",
    "    best_sequence = None\n",
    "    best_prob = 0.0\n",
    "    for perm in itertools.permutations(words):\n",
    "        prob = calculate_sequence_probability(perm, ngrams)\n",
    "        if prob > best_prob:\n",
    "            best_prob = prob\n",
    "            best_sequence = perm\n",
    "    return best_sequence, best_prob\n",
    "\n",
    "# 예측 예제\n",
    "input_words = [\"기술\", \"데이터\", \"안전\", \"분석\"]\n",
    "best_sequence, best_prob = find_best_sequence(input_words, ngrams)\n",
    "\n",
    "print(\"Best sequence:\", best_sequence)\n",
    "print(\"Probability:\", best_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m okt \u001b[38;5;241m=\u001b[39m Okt()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# 사전 훈련된 한국어 언어 모델 로드 (예: KoBERT)\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m unmasker \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfill-mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmonologg/kobert\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_valid_combination\u001b[39m(noun1, noun2):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# 두 명사를 결합한 문장을 생성\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnoun1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnoun2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m는\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\interX_work\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:895\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    894\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m--> 895\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    905\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    906\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\interX_work\\Lib\\site-packages\\transformers\\pipelines\\base.py:234\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;124;03mSelect framework (TensorFlow or PyTorch) to use from the `model` passed. Returns a tuple (framework, model).\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;124;03m    `Tuple`: A tuple framework, model.\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tf_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available():\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one of TensorFlow 2.0 or PyTorch should be installed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo install PyTorch, read the instructions at https://pytorch.org/.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    238\u001b[0m     )\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    240\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_pipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m task\n",
      "\u001b[1;31mRuntimeError\u001b[0m: At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/."
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from soynlp.word import WordExtractor\n",
    "from transformers import pipeline\n",
    "\n",
    "# 한국어 형태소 분석기 초기화\n",
    "okt = Okt()\n",
    "\n",
    "# 사전 훈련된 한국어 언어 모델 로드 (예: KoBERT)\n",
    "unmasker = pipeline('fill-mask', model='monologg/kobert')\n",
    "\n",
    "def is_valid_combination(noun1, noun2):\n",
    "    # 두 명사를 결합한 문장을 생성\n",
    "    sentence = f\"{noun1} {noun2}는\"\n",
    "    masked_sentence = sentence.replace(noun2, '[MASK]')\n",
    "    \n",
    "    # 언어 모델을 사용하여 [MASK] 위치의 단어 예측\n",
    "    result = unmasker(masked_sentence)\n",
    "    \n",
    "    # 가장 높은 확률의 단어가 noun2와 일치하는지 확인\n",
    "    for prediction in result:\n",
    "        if prediction['token_str'] == noun2:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# 테스트할 명사 리스트\n",
    "nouns = ['작동', '계획']\n",
    "\n",
    "# 모든 조합에 대해 올바른 순서인지 판단\n",
    "for i in range(len(nouns)):\n",
    "    for j in range(len(nouns)):\n",
    "        if i != j:\n",
    "            noun1 = nouns[i]\n",
    "            noun2 = nouns[j]\n",
    "            if is_valid_combination(noun1, noun2):\n",
    "                print(f\"'{noun1} {noun2}'는 올바른 조합입니다.\")\n",
    "            else:\n",
    "                print(f\"'{noun1} {noun2}'는 올바르지 않은 조합입니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\envs\\interX_work\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\PC\\anaconda3\\envs\\interX_work\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\PC\\.cache\\huggingface\\hub\\models--monologg--kobert. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'작동 계획'는 올바르지 않은 조합입니다.\n",
      "'계획 작동'는 올바르지 않은 조합입니다.\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from transformers import pipeline\n",
    "\n",
    "# 한국어 형태소 분석기 초기화\n",
    "okt = Okt()\n",
    "\n",
    "# 사전 훈련된 한국어 언어 모델 로드 (예: KoBERT)\n",
    "unmasker = pipeline('fill-mask', model='monologg/kobert')\n",
    "\n",
    "def is_valid_combination(noun1, noun2):\n",
    "    # 두 명사를 결합한 문장을 생성\n",
    "    sentence = f\"{noun1} {noun2}는\"\n",
    "    masked_sentence = sentence.replace(noun2, '[MASK]')\n",
    "    \n",
    "    # 언어 모델을 사용하여 [MASK] 위치의 단어 예측\n",
    "    result = unmasker(masked_sentence)\n",
    "    \n",
    "    # 가장 높은 확률의 단어가 noun2와 일치하는지 확인\n",
    "    for prediction in result:\n",
    "        if prediction['token_str'] == noun2:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# 테스트할 명사 리스트\n",
    "nouns = ['작동', '계획']\n",
    "\n",
    "# 모든 조합에 대해 올바른 순서인지 판단\n",
    "for i in range(len(nouns)):\n",
    "    for j in range(len(nouns)):\n",
    "        if i != j:\n",
    "            noun1 = nouns[i]\n",
    "            noun2 = nouns[j]\n",
    "            if is_valid_combination(noun1, noun2):\n",
    "                print(f\"'{noun1} {noun2}'는 올바른 조합입니다.\")\n",
    "            else:\n",
    "                print(f\"'{noun1} {noun2}'는 올바르지 않은 조합입니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\pc\\anaconda3\\envs\\interx_work\\lib\\site-packages (2.3.1)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.18.1-cp311-cp311-win_amd64.whl.metadata (6.6 kB)\n",
      "Collecting torchaudio\n",
      "  Using cached torchaudio-2.3.1-cp311-cp311-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\pc\\anaconda3\\envs\\interx_work\\lib\\site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\pc\\anaconda3\\envs\\interx_work\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\pc\\anaconda3\\envs\\interx_work\\lib\\site-packages (from torch) (1.13.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\pc\\anaconda3\\envs\\interx_work\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pc\\anaconda3\\envs\\interx_work\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\pc\\anaconda3\\envs\\interx_work\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\pc\\anaconda3\\envs\\interx_work\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\pc\\anaconda3\\envs\\interx_work\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\pc\\anaconda3\\envs\\interx_work\\lib\\site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\pc\\anaconda3\\envs\\interx_work\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\pc\\anaconda3\\envs\\interx_work\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.13.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pc\\anaconda3\\envs\\interx_work\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\pc\\anaconda3\\envs\\interx_work\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading torchvision-0.18.1-cp311-cp311-win_amd64.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 0.5/1.2 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.2/1.2 MB 9.4 MB/s eta 0:00:00\n",
      "Downloading torchaudio-2.3.1-cp311-cp311-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ------------------------ --------------- 1.4/2.4 MB 45.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 30.2 MB/s eta 0:00:00\n",
      "Installing collected packages: torchvision, torchaudio\n",
      "Successfully installed torchaudio-2.3.1 torchvision-0.18.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'작동 계획'는 올바르지 않은 조합입니다.\n",
      "'계획 작동'는 올바르지 않은 조합입니다.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "# KoBERT 모델과 토크나이저 로드\n",
    "tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n",
    "model = BertForMaskedLM.from_pretrained('monologg/kobert')\n",
    "\n",
    "def is_valid_combination(noun1, noun2):\n",
    "    # 두 명사를 결합한 문장을 생성\n",
    "    sentence = f\"{noun1} {noun2}는\"\n",
    "    masked_sentence = sentence.replace(noun2, '[MASK]')\n",
    "    \n",
    "    # 토크나이저를 사용하여 문장 토큰화\n",
    "    inputs = tokenizer(masked_sentence, return_tensors='pt')\n",
    "    \n",
    "    # 모델을 사용하여 [MASK] 위치의 단어 예측\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    predictions = outputs.logits\n",
    "\n",
    "    # [MASK] 토큰의 인덱스 추출\n",
    "    mask_token_index = (inputs.input_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
    "\n",
    "    # [MASK] 토큰 위치의 단어 예측\n",
    "    mask_token_logits = predictions[0, mask_token_index, :]\n",
    "    top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "    # 가장 높은 확률의 단어가 noun2와 일치하는지 확인\n",
    "    predicted_tokens = tokenizer.convert_ids_to_tokens(top_5_tokens)\n",
    "    if noun2 in predicted_tokens:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# 테스트할 명사 리스트\n",
    "nouns = ['작동', '계획']\n",
    "\n",
    "# 모든 조합에 대해 올바른 순서인지 판단\n",
    "for i in range(len(nouns)):\n",
    "    for j in range(len(nouns)):\n",
    "        if i != j:\n",
    "            noun1 = nouns[i]\n",
    "            noun2 = nouns[j]\n",
    "            if is_valid_combination(noun1, noun2):\n",
    "                print(f\"'{noun1} {noun2}'는 올바른 조합입니다.\")\n",
    "            else:\n",
    "                print(f\"'{noun1} {noun2}'는 올바르지 않은 조합입니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "올바른 명사 조합:\n",
      "계획 작동: 2회\n",
      "'작동 계획'는 올바르지 않은 조합입니다.\n",
      "'계획 작동'는 올바른 조합입니다.\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# 형태소 분석기 초기화\n",
    "okt = Okt()\n",
    "\n",
    "# 예제 텍스트 데이터 (여기서는 간단한 예제만 사용하지만, 실제로는 대규모 코퍼스를 사용해야 합니다)\n",
    "text_data = \"\"\"\n",
    "계획 작동은 중요한 과정입니다. 작동 계획이 제대로 이루어져야 합니다.\n",
    "계획을 작동하는 방법은 여러 가지가 있습니다. 좋은 계획은 성공적인 작동으로 이어집니다.\n",
    "\"\"\"\n",
    "\n",
    "# 명사 추출 함수\n",
    "def extract_nouns(text):\n",
    "    sentences = text.split('\\n')\n",
    "    nouns = []\n",
    "    for sentence in sentences:\n",
    "        tokens = okt.pos(sentence, norm=True, stem=True)\n",
    "        nouns.extend([word for word, pos in tokens if pos == 'Noun'])\n",
    "    return nouns\n",
    "\n",
    "# 명사 쌍 추출 함수\n",
    "def extract_noun_pairs(nouns):\n",
    "    pairs = []\n",
    "    for i in range(len(nouns) - 1):\n",
    "        pairs.append((nouns[i], nouns[i+1]))\n",
    "    return pairs\n",
    "\n",
    "# 명사 추출\n",
    "nouns = extract_nouns(text_data)\n",
    "\n",
    "# 명사 쌍 추출\n",
    "noun_pairs = extract_noun_pairs(nouns)\n",
    "\n",
    "# 명사 쌍 빈도 계산\n",
    "pair_counts = Counter(noun_pairs)\n",
    "\n",
    "# 일정 빈도 이상의 명사 쌍만 \"올바른 조합\"으로 간주\n",
    "threshold = 1\n",
    "valid_pairs = {pair: count for pair, count in pair_counts.items() if count > threshold}\n",
    "\n",
    "# 결과 출력\n",
    "print(\"올바른 명사 조합:\")\n",
    "for pair, count in valid_pairs.items():\n",
    "    print(f\"{pair[0]} {pair[1]}: {count}회\")\n",
    "\n",
    "# 테스트할 명사 리스트\n",
    "nouns_to_test = ['작동', '계획']\n",
    "\n",
    "# 테스트 함수\n",
    "def is_valid_combination(noun1, noun2):\n",
    "    return (noun1, noun2) in valid_pairs\n",
    "\n",
    "# 모든 조합에 대해 올바른 순서인지 판단\n",
    "for i in range(len(nouns_to_test)):\n",
    "    for j in range(len(nouns_to_test)):\n",
    "        if i != j:\n",
    "            noun1 = nouns_to_test[i]\n",
    "            noun2 = nouns_to_test[j]\n",
    "            if is_valid_combination(noun1, noun2):\n",
    "                print(f\"'{noun1} {noun2}'는 올바른 조합입니다.\")\n",
    "            else:\n",
    "                print(f\"'{noun1} {noun2}'는 올바르지 않은 조합입니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트할 명사 조합:\n",
      "'('작동', '계획')'는 올바르지 않은 조합입니다.\n",
      "'('작동', '설계')'는 올바르지 않은 조합입니다.\n",
      "'('작동', '운영')'는 올바르지 않은 조합입니다.\n",
      "'('작동', '안전')'는 올바르지 않은 조합입니다.\n",
      "'('작동', '검토')'는 올바르지 않은 조합입니다.\n",
      "'('계획', '설계')'는 올바르지 않은 조합입니다.\n",
      "'('계획', '운영')'는 올바르지 않은 조합입니다.\n",
      "'('계획', '안전')'는 올바르지 않은 조합입니다.\n",
      "'('계획', '검토')'는 올바르지 않은 조합입니다.\n",
      "'('설계', '운영')'는 올바르지 않은 조합입니다.\n",
      "'('설계', '안전')'는 올바르지 않은 조합입니다.\n",
      "'('설계', '검토')'는 올바르지 않은 조합입니다.\n",
      "'('운영', '안전')'는 올바르지 않은 조합입니다.\n",
      "'('운영', '검토')'는 올바르지 않은 조합입니다.\n",
      "'('안전', '검토')'는 올바르지 않은 조합입니다.\n",
      "'('작동', '계획', '설계')'는 올바르지 않은 조합입니다.\n",
      "'('작동', '계획', '운영')'는 올바르지 않은 조합입니다.\n",
      "'('작동', '계획', '안전')'는 올바르지 않은 조합입니다.\n",
      "'('작동', '계획', '검토')'는 올바르지 않은 조합입니다.\n",
      "'('작동', '설계', '운영')'는 올바르지 않은 조합입니다.\n",
      "'('작동', '설계', '안전')'는 올바르지 않은 조합입니다.\n",
      "'('작동', '설계', '검토')'는 올바르지 않은 조합입니다.\n",
      "'('작동', '운영', '안전')'는 올바르지 않은 조합입니다.\n",
      "'('작동', '운영', '검토')'는 올바르지 않은 조합입니다.\n",
      "'('작동', '안전', '검토')'는 올바르지 않은 조합입니다.\n",
      "'('계획', '설계', '운영')'는 올바르지 않은 조합입니다.\n",
      "'('계획', '설계', '안전')'는 올바르지 않은 조합입니다.\n",
      "'('계획', '설계', '검토')'는 올바르지 않은 조합입니다.\n",
      "'('계획', '운영', '안전')'는 올바르지 않은 조합입니다.\n",
      "'('계획', '운영', '검토')'는 올바르지 않은 조합입니다.\n",
      "'('계획', '안전', '검토')'는 올바르지 않은 조합입니다.\n",
      "'('설계', '운영', '안전')'는 올바르지 않은 조합입니다.\n",
      "'('설계', '운영', '검토')'는 올바르지 않은 조합입니다.\n",
      "'('설계', '안전', '검토')'는 올바르지 않은 조합입니다.\n",
      "'('운영', '안전', '검토')'는 올바르지 않은 조합입니다.\n",
      "'('작동', '계획', '설계', '운영')'는 올바르지 않은 조합입니다.\n",
      "'('작동', '계획', '설계', '안전')'는 올바르지 않은 조합입니다.\n",
      "'('작동', '계획', '설계', '검토')'는 올바르지 않은 조합입니다.\n",
      "'('작동', '계획', '운영', '안전')'는 올바르지 않은 조합입니다.\n",
      "'('작동', '계획', '운영', '검토')'는 올바르지 않은 조합입니다.\n",
      "'('작동', '계획', '안전', '검토')'는 올바르지 않은 조합입니다.\n",
      "'('작동', '설계', '운영', '안전')'는 올바르지 않은 조합입니다.\n",
      "'('작동', '설계', '운영', '검토')'는 올바르지 않은 조합입니다.\n",
      "'('작동', '설계', '안전', '검토')'는 올바르지 않은 조합입니다.\n",
      "'('작동', '운영', '안전', '검토')'는 올바르지 않은 조합입니다.\n",
      "'('계획', '설계', '운영', '안전')'는 올바르지 않은 조합입니다.\n",
      "'('계획', '설계', '운영', '검토')'는 올바르지 않은 조합입니다.\n",
      "'('계획', '설계', '안전', '검토')'는 올바르지 않은 조합입니다.\n",
      "'('계획', '운영', '안전', '검토')'는 올바르지 않은 조합입니다.\n",
      "'('설계', '운영', '안전', '검토')'는 올바르지 않은 조합입니다.\n",
      "'('작동', '계획', '설계', '운영', '안전')'는 올바르지 않은 조합입니다.\n",
      "'('작동', '계획', '설계', '운영', '검토')'는 올바르지 않은 조합입니다.\n",
      "'('작동', '계획', '설계', '안전', '검토')'는 올바르지 않은 조합입니다.\n",
      "'('작동', '계획', '운영', '안전', '검토')'는 올바르지 않은 조합입니다.\n",
      "'('작동', '설계', '운영', '안전', '검토')'는 올바르지 않은 조합입니다.\n",
      "'('계획', '설계', '운영', '안전', '검토')'는 올바르지 않은 조합입니다.\n",
      "'('작동', '계획', '설계', '운영', '안전', '검토')'는 올바르지 않은 조합입니다.\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "import itertools\n",
    "\n",
    "# KoBERT 모델과 토크나이저 로드\n",
    "tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n",
    "model = BertForMaskedLM.from_pretrained('monologg/kobert')\n",
    "\n",
    "# 형태소 분석기 초기화\n",
    "okt = Okt()\n",
    "\n",
    "# 명사 조합의 적절성을 평가하는 함수\n",
    "def is_valid_combination(nouns):\n",
    "    sentence = \" \".join(nouns) + \"는\"\n",
    "    valid = True\n",
    "    for i in range(len(nouns)):\n",
    "        masked_sentence = sentence.replace(nouns[i], '[MASK]')\n",
    "        inputs = tokenizer(masked_sentence, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        predictions = outputs.logits\n",
    "        mask_token_index = (inputs.input_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
    "        mask_token_logits = predictions[0, mask_token_index, :]\n",
    "        top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "        predicted_tokens = tokenizer.convert_ids_to_tokens(top_5_tokens)\n",
    "        if nouns[i] not in predicted_tokens:\n",
    "            valid = False\n",
    "            break\n",
    "    return valid\n",
    "\n",
    "# 테스트할 명사 리스트\n",
    "nouns_to_test = ['작동', '계획', '설계', '운영', '안전', '검토']\n",
    "\n",
    "# 모든 조합에 대해 올바른 순서인지 판단 (최대 6개 명사까지)\n",
    "print(\"테스트할 명사 조합:\")\n",
    "for length in range(2, 7):\n",
    "    for combination in itertools.combinations(nouns_to_test, length):\n",
    "        if is_valid_combination(combination):\n",
    "            print(f\"'{combination}'는 올바른 조합입니다.\")\n",
    "        else:\n",
    "            print(f\"'{combination}'는 올바르지 않은 조합입니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 '계획'의 품사 분석 결과:\n",
      " - 계획: Noun\n",
      " - 계획: Noun\n",
      "단어 '작동'의 품사 분석 결과:\n",
      " - 작동: Noun\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "# 형태소 분석기 초기화\n",
    "okt = Okt()\n",
    "\n",
    "# 문장에서 단어의 품사를 분석하는 함수\n",
    "def analyze_pos(text, target_word):\n",
    "    pos_tags = okt.pos(text)\n",
    "    results = []\n",
    "    for word, pos in pos_tags:\n",
    "        if word == target_word:\n",
    "            results.append((word, pos))\n",
    "    return results\n",
    "\n",
    "# 테스트할 문장과 단어\n",
    "sentence = \"계획을 잘 세워야 합니다. 작동하는 기계를 점검하세요. 계획된 작업을 수행하십시오.\"\n",
    "target_words = [\"계획\", \"작동\"]\n",
    "\n",
    "# 각 단어가 문장에서 어떤 품사로 사용되었는지 분석\n",
    "for word in target_words:\n",
    "    pos_analysis = analyze_pos(sentence, word)\n",
    "    print(f\"단어 '{word}'의 품사 분석 결과:\")\n",
    "    for analysis in pos_analysis:\n",
    "        print(f\" - {analysis[0]}: {analysis[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interX_work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
