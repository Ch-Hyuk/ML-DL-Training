{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CDD 단어 추출\n",
    "- CDD에서 정의된 용어, 단어는 검증이 완료되었다고 판단\n",
    "- 각 데이터 에서 단어를 최대한 많이 뽑아내는 것이 목적\n",
    "\n",
    "### CDD class data\n",
    "- 정리된 CDD class excel 파일 3212 개 \n",
    "- Class단위의 PreferredName에서 단어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\Desktop\\git_repo\\ML-DL-Training\\Data_preprocess\\workspace\\file\n"
     ]
    }
   ],
   "source": [
    "#라이브러리 import\n",
    "\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "dir_path = os.getcwd()\n",
    "dir_path = os.path.join(dir_path, \"file\")\n",
    "print(dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1차 데이터 처리 작업\n",
    "\n",
    "- CDD 클래스 정리 파일 로딩\n",
    "- 토큰화를 통한 단어 분리\n",
    "- 특수 문자 제거 및 숫자 제거\n",
    "- 불용어 제거, 길이가 2이하인 단어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 경로와 읽을 열 이름\n",
    "file_name = \"CDD_class.xlsx\"\n",
    "file_path = os.path.join(dir_path,file_name)  \n",
    "column_name = 'PreferredName'  \n",
    "\n",
    "# 엑셀 파일 읽기\n",
    "df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#nltk 라이브러리 import\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#불용어\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 각 기능별 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 품사태그를 WordNet의 태그로 변환\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Definition 문장 전처리 함수\n",
    "def process_sentence(sentence):\n",
    "    # 문장 토큰화\n",
    "    tokens = word_tokenize(sentence)\n",
    "    # 특수문자 제거\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # 품사 태깅\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    # 불용어 리스트 가져오기\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Lemmatizer 객체 생성\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # 불용어 제거 및 단어의 원형 추출\n",
    "    lemmatized_tokens = [\n",
    "        lemmatizer.lemmatize(word, pos=get_wordnet_pos(tag) or nltk.corpus.wordnet.NOUN)\n",
    "        for word, tag in tagged_tokens if word.lower() not in stop_words and len(word) > 2\n",
    "    ]\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#각 column 데이터에 따른 데이터프레임 생성\n",
    "def word_df_create(column_name):\n",
    "\n",
    "    # 해당 열의 단어 추출 및 처리\n",
    "    all_words = []\n",
    "    for sentence in df[column_name].dropna():\n",
    "        words = process_sentence(sentence)\n",
    "        all_words.extend(words)\n",
    "\n",
    "    # 결과를 데이터프레임으로 변환\n",
    "    result_df = pd.DataFrame(set(all_words), columns=['Word'])\n",
    "\n",
    "    return result_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#column_name: Preferred name -> pre_df 데이터 프레임 생성\n",
    "\n",
    "pre_df = word_df_create(column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Proximity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Restriction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Circular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>compensation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>cradle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1203</th>\n",
       "      <td>viscosity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>setpoint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205</th>\n",
       "      <td>Installation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>Conditions</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1207 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Word\n",
       "0        Proximity\n",
       "1              Bus\n",
       "2      Restriction\n",
       "3         Circular\n",
       "4     compensation\n",
       "...            ...\n",
       "1202        cradle\n",
       "1203     viscosity\n",
       "1204      setpoint\n",
       "1205  Installation\n",
       "1206    Conditions\n",
       "\n",
       "[1207 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2차 데이터 처리 작업\n",
    "\n",
    "- 대문자 중복 단어 제거\n",
    "- 축약어, 단어 분리 처리\n",
    "- 단어 표제어 변환\n",
    "- 각 column 별 데이터 추출 후 병합, 중복 처리\n",
    "- 최종 데이터 엑셀파일 저장   \n",
    " **-> sheet1: 단어 데이터, sheet2: 대문자 축약어 데이터**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#대문자 중복 단어 제거 및 축약어와 단어의 분리\n",
    "def Cap_Dup_process(result_df):\n",
    "\n",
    "    column_data = result_df['Word'].dropna()\n",
    "\n",
    "    upper_data = pd.DataFrame()\n",
    "    lower_data = pd.DataFrame()\n",
    "\n",
    "    # pattern: 대문자 2개 이상 or 첫 문자가 소문자이면서 이외의 글자가 대문자인 경우\n",
    "    pattern = r'^(?=.*[A-Z].*[A-Z])|^(?:[a-z].*[A-Z])'\n",
    "\n",
    "    # 패턴의 포함 여부에 따라 분류\n",
    "    upper_data['Word'] = column_data[column_data.str.contains(pattern, regex=True)]             \n",
    "    lower_data['Word'] = column_data[~column_data.str.contains(pattern, regex=True, na=False)]  \n",
    "\n",
    "    # 소문자 통일 처리\n",
    "    upper_data['UP_Lower'] = upper_data['Word'].str.lower().to_frame()\n",
    "    lower_data['LO_Lower'] = lower_data['Word'].str.lower().to_frame()\n",
    "\n",
    "    # lower_data를 기준으로 inner merge\n",
    "    merged_df = pd.merge(upper_data, lower_data, left_on='UP_Lower', right_on='LO_Lower', how='inner')\n",
    "\n",
    "    # upper_data 중 중복이 아닌 데이터 분리\n",
    "    upper_only_data = upper_data.loc[~upper_data['UP_Lower'].isin(merged_df['UP_Lower']), ['Word']] \n",
    "\n",
    "    # lower_data 중 중복이 아닌 데이터 분리\n",
    "    lower_only_data = lower_data.loc[~lower_data['LO_Lower'].isin(merged_df['LO_Lower']), ['Word']] \n",
    "\n",
    "    # overlap data 정리\n",
    "    overlap_data = merged_df[['Word_y']].rename(columns={'Word_y': 'Word'})                        \n",
    "\n",
    "    # word data 정리\n",
    "    word_data = pd.concat([lower_only_data, overlap_data], ignore_index=True)                       \n",
    "    word_data['Word'] = word_data['Word'].str.lower().drop_duplicates()\n",
    "\n",
    "    # 결측치 처리 및 index 재정의\n",
    "    return word_data.dropna().reset_index(drop=True), upper_only_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#각 column데이터 별로 단어 df, 대문자 df로 분리\n",
    "\n",
    "pre_word_df, pre_upper_df = Cap_Dup_process(pre_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 표제어 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob, Word\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet, words\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('words')\n",
    "\n",
    "EN_words = set(words.words())\n",
    "\n",
    "import inflect\n",
    "\n",
    "p = inflect.engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steemmer를 사용하여 단어의 원형 추출\n",
    "def stemmer(word):\n",
    "    ps = PorterStemmer()\n",
    "    return ps.stem(word)\n",
    "\n",
    "# 단수형 변환 함수 정의\n",
    "def to_singular(word):\n",
    "    singular_word = p.singular_noun(word)\n",
    "    return singular_word if singular_word else word\n",
    "\n",
    "# 영어 단어 인지 확인\n",
    "def is_EN_word(word):\n",
    "    if word in EN_words:\n",
    "        return word\n",
    "    \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# 원형을 추출\n",
    "def get_lemma(word):\n",
    "    pos = pos_tag([word])[0][1]  # 단어의 품사 태깅\n",
    "    wordnet_pos = get_wordnet_pos(pos)  # WordNet의 품사 태그로 변환\n",
    "    lemma = lemmatizer.lemmatize(word, wordnet_pos)  # 원형 추출\n",
    "    return lemma\n",
    "\n",
    "\n",
    "# 2이하인 값 제거\n",
    "def remove_short_words(cell):\n",
    "    return cell if len(cell) > 2 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표제어 추출\n",
    "def Lemmatization_process(word_data):\n",
    "    # 데이터프레임에 단어의 원형 추출\n",
    "    word_result = pd.DataFrame()\n",
    "\n",
    "    # 추출 단어의 단수형 추출, nltk 내의 word에서 존재여부 파악, 중복 처리\n",
    "    word_result['Word'] = word_data['Word'].apply(lambda x: is_EN_word(to_singular(x))).dropna().drop_duplicates()\n",
    "\n",
    "    # 추출 단어의 어간 추출(Stemming)\n",
    "    word_result['Stemm_Word'] = word_result['Word'].apply(stemmer)\n",
    "\n",
    "    # 어간 추출 단어와 기존 단어의 비교\n",
    "    word_result['Data_Equal'] = word_result.eval('Word == Stemm_Word')\n",
    "\n",
    "    # 어간 추출 단어와 다른 단어중 nltk 내의 word에서 존재여부 파악 \n",
    "    Stemm_Word = word_result[word_result['Data_Equal'] == False][['Stemm_Word']]\n",
    "    word_result['Stem_con'] = Stemm_Word['Stemm_Word'].apply(is_EN_word)\n",
    "\n",
    "    # 최종 결과 col생성\n",
    "    word_result['Result_word'] = word_result.apply(\n",
    "        lambda row: row['Word'] if row['Data_Equal'] else (row['Stemm_Word'] if row['Stem_con'] else row['Word']),axis=1\n",
    "    )\n",
    "\n",
    "    # 2이하인 값 제거\n",
    "    word_result['Result_word'] = word_result['Result_word'].apply(remove_short_words).dropna()\n",
    "\n",
    "    return word_result['Result_word'].drop_duplicates().to_frame(name='Word').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 단어 추출\n",
    "word_result = Lemmatization_process(pre_word_df).dropna()\n",
    "\n",
    "upper_result = pre_upper_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>proximity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>restrict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>circular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>compensation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>attach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>cradle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>viscosity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>input</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>output</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>measurement</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>766 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Word\n",
       "0       proximity\n",
       "2        restrict\n",
       "3        circular\n",
       "4    compensation\n",
       "5          attach\n",
       "..            ...\n",
       "762        cradle\n",
       "763     viscosity\n",
       "764         input\n",
       "765        output\n",
       "766   measurement\n",
       "\n",
       "[766 rows x 1 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SPD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>READBACK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DLOP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SIMULATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RTD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DIP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CNG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LOP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LOQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NAMUR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>PQI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>OLOP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LOPD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>IEC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word\n",
       "0        SPD\n",
       "1   READBACK\n",
       "2       DLOP\n",
       "3        PTC\n",
       "4        LPG\n",
       "5       HART\n",
       "6   SIMULATE\n",
       "7        LED\n",
       "8        RTD\n",
       "9        DIP\n",
       "10       CNG\n",
       "11       PMD\n",
       "12       RMS\n",
       "13       LOP\n",
       "14       LOQ\n",
       "15     NAMUR\n",
       "16       PQI\n",
       "17      OLOP\n",
       "18      LOPD\n",
       "19       IEC"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 엑셀 시트에 분할 저장\n",
    "\n",
    "result_file_path = os.path.join(dir_path,'CDD_Word_Data.xlsx')\n",
    "with pd.ExcelWriter(result_file_path) as writer:\n",
    "    word_result.to_excel(writer, sheet_name='Sheet1', index=False)\n",
    "    upper_result.to_excel(writer, sheet_name='Sheet2', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interX_work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
