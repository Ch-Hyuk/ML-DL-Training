{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\Desktop\\git_repo\\ML-DL-Training\\Data_preprocess\\workspace\\file\n"
     ]
    }
   ],
   "source": [
    "#라이브러리 import\n",
    "\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "dir_path = os.getcwd()\n",
    "dir_path = os.path.join(dir_path, \"file\")\n",
    "print(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppt_file_load(dir_path, file_name):\n",
    "    file_path = os.path.join(dir_path,file_name)\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#nltk 라이브러리 import\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#불용어\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텍스트가 성공적으로 추출되었습니다.\n"
     ]
    }
   ],
   "source": [
    "from pptx import Presentation\n",
    "\n",
    "def extract_text_from_pptx(pptx_path):\n",
    "    prs = Presentation(pptx_path)\n",
    "    text_runs = []\n",
    "    \n",
    "    for slide in prs.slides:\n",
    "        for shape in slide.shapes:\n",
    "            if not shape.has_text_frame:\n",
    "                continue\n",
    "            for paragraph in shape.text_frame.paragraphs:\n",
    "                for run in paragraph.runs:\n",
    "                    text_runs.append(run.text)\n",
    "    \n",
    "    return text_runs\n",
    "\n",
    "file_name = 'test_ppt.pptx'  # 여기에 파워포인트 파일 경로를 입력하세요\n",
    "pptx_path = ppt_file_load(dir_path,file_name)\n",
    "text_runs = extract_text_from_pptx(pptx_path)\n",
    "\n",
    "# 추출한 텍스트를 파일로 저장 (선택 사항)\n",
    "with open('extracted_text.txt', 'w', encoding='utf-8') as f:\n",
    "    for text in text_runs:\n",
    "        f.write(text + '\\n')\n",
    "\n",
    "print(\"텍스트가 성공적으로 추출되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텍스트가 성공적으로 추출되어 extracted_words.xlsx 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pptx import Presentation\n",
    "import pandas as pd\n",
    "\n",
    "def clean_and_split_text(text_list):\n",
    "    cleaned_text = []\n",
    "    for text in text_list:\n",
    "        # 숫자와 특수문자를 공백으로 변환 (한글과 영어만 남기기)\n",
    "        cleaned = re.sub(r'[^가-힣a-zA-Z\\s]', ' ', text)\n",
    "        # 여러 개의 공백을 하나의 공백으로 변환\n",
    "        cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "        # 공백을 기준으로 분리하여 리스트에 추가\n",
    "        cleaned_text.extend(cleaned.strip().split(' '))\n",
    "    \n",
    "    # 빈 문자열을 필터링 및 중복 제거, 한 글자 이하 단어 제거, 영어 불용어 제거, 연속된 'x'가 두 개 이상인 단어 제거\n",
    "    cleaned_text = {word for word in cleaned_text if len(word) > 1 and (word.isalpha() and word.lower() not in stop_words) and not re.search(r'xx+', word.lower()) or bool(re.search(r'[가-힣]', word))}\n",
    "    return list(cleaned_text)\n",
    "\n",
    "cleaned_words = clean_and_split_text(text_runs)\n",
    "\n",
    "# 데이터를 데이터프레임으로 변환\n",
    "df = pd.DataFrame(cleaned_words, columns=['Word'])\n",
    "\n",
    "# 엑셀 파일로 저장\n",
    "excel_path = 'extracted_words.xlsx'\n",
    "df.to_excel(excel_path, index=False)\n",
    "\n",
    "print(f\"텍스트가 성공적으로 추출되어 {excel_path} 파일로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interX_work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
